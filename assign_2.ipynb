{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                        Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING REQUIRED MODULES\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import math\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADIENT DESCENT FUNCTION GIVEN IN ASSIGNMENT\n",
    "\n",
    "def gradient_descent(gradient,init_,learn_rate,n_iter=100,tol=1e-06):\n",
    "    x=init_\n",
    "    for i in range(n_iter):\n",
    "        delta= -learn_rate * gradient(x)\n",
    "        if np.all(np.abs(delta)<=tol):\n",
    "            break\n",
    "        x+=delta\n",
    "    return round(x*1000)/1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.a(i) finding minima for $f(x) = x^2+3x+4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minima of function x^2 + 3x +4 is:1.75 at x value:-1.5\n"
     ]
    }
   ],
   "source": [
    "#VARIABLES TO STORE THE MINIMA AND VALUE OF X AT WHICH THE MINIMA OCCURS\n",
    "\n",
    "min_x=0\n",
    "minima=100000\n",
    "\n",
    "#CALCULATING THE GRADIENT DESCENT FOR DIFFERENT LEARNING RATES(i*0.001 is the learning rate for that iteration)\n",
    "\n",
    "for i in range(0,50):\n",
    "    \n",
    "    #GRADIENT FOR X^2+3X+4 IS 2X+3(THIS IS DEFINED AS GRADIENT FUNCTION)\n",
    "    #CALLING GRADIENT DESCENT FUNCTION\n",
    "    #STORING THE X VALUE RETURNED BY GRADIENT DESCENT IN current_x \n",
    "    \n",
    "    current_x=gradient_descent(gradient=lambda v: 2 * v + 3, init_=4, learn_rate=0.001*i)\n",
    "    \n",
    "    #CALCULATING THE VALUE OF X^2+3X+4 FOR THAT current_x VALUE AND STORING IT IN current_minima\n",
    "    \n",
    "    current_minima=(current_x)**2+3*current_x+4\n",
    "    \n",
    "    #IF CURRENT MINIMA IS LESS THAN OVERALL MINIMA CALCULATED TILL THAT ITERATION THEN UPDATING\n",
    "    #MINIMA AS CURRENT MINIMA AND UPDATING THE MIN_X VALUE VALUE TO X VALUE CORRESPONDING TO THAT CURRENT MINIMA\n",
    "    \n",
    "    if current_minima<=minima:\n",
    "        min_x=current_x\n",
    "        minima=current_minima\n",
    "\n",
    "#PRINTING THE OVERALL MINIMA OF X^2+3X+4 AND THE VALUE X WHERE THE MINIMA OCCURS\n",
    "\n",
    "print('minima of function x^2 + 3x +4 is:'+str(minima)+' at x value:'+str(min_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.a(ii) finding minima for $f(x) = x^4-3x^2+2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minima of function x^4-3x^2+2x is:-4.848076206064 at x value:-1.366\n"
     ]
    }
   ],
   "source": [
    "#VARIABLES TO STORE THE MINIMA AND VALUE OF X AT WHICH THE MINIMA OCCURS\n",
    "\n",
    "min_x=0\n",
    "minima=100000\n",
    "\n",
    "#CALCULATING THE GRADIENT DESCENT FOR DIFFERENT LEARNING RATES(i*0.001 is the learning rate for that iteration)\n",
    "\n",
    "for i in range(1,30):\n",
    "    \n",
    "    #GRADIENT FOR X^4-3X^2+2X IS 4X^3-6X+2(THIS IS DEFINED AS GRADIENT FUNCTION)\n",
    "    #CALLING GRADIENT DESCENT FUNCTION\n",
    "    #STORING THE X VALUE RETURNED BY GRADIENT DESCENT IN current_x \n",
    "    \n",
    "    current_x=gradient_descent(gradient=lambda v:  4* v*v*v-6*v + 2, init_=4.0, learn_rate=0.001*i)\n",
    "    \n",
    "     #CALCULATING THE VALUE OF X^4-3X^2+2X FOR THAT current_x VALUE AND STORING IT IN current_minima\n",
    "    \n",
    "    current_minima=(current_x)**4-3*(current_x)**2+2*current_x\n",
    "    \n",
    "    #IF CURRENT MINIMA IS LESS THAN OVERALL MINIMA CALCULATED TILL THAT ITERATION THEN UPDATING\n",
    "    #MINIMA AS CURRENT MINIMA AND UPDATING THE MIN_X VALUE VALUE TO X VALUE CORRESPONDING TO THAT CURRENT MINIMA\n",
    "    \n",
    "    if current_minima<=minima:\n",
    "        min_x=current_x\n",
    "        minima=current_minima\n",
    "        \n",
    "#PRINTING THE OVERALL MINIMA OF X^4-3X^2+2X  AND THE VALUE X WHERE THE MINIMA OCCURS\n",
    "\n",
    "print('minima of function x^4-3x^2+2x is:'+str(minima)+' at x value:'+str(min_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.b gradient function to calculate gradients for a linear regression y = ax+b\n",
    "# Loss Function $L(a,b) = \\frac{1}{2n} \\sum_{i=1}^n (y_{i}-ax_{i}-b)^2$\n",
    "# $\\frac{\\partial (L)}{\\partial a} = \\frac{1}{n} \\sum_{i=1}^n (y_{i}-ax_{i}-b)(-x_{i})$\n",
    "# $\\frac{\\partial (L)}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n -(y_{i}-ax_{i}-b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADIENT FUNCTION TO CALCULATE GRADIENTS FOR LINEAR REGRESSION(y=ax+b)\n",
    "\n",
    "#FUNCTION TAKES PARAMETERS W(WEIGHTS VECTOR),X(INPUT FEATURES MATRIX),Y(OUTPUT FEATURE MATRIX)\n",
    "\n",
    "def gradient(w,X,Y):\n",
    "    \n",
    "    #CALCULATING THE PREDICTED OUTPUT MATRIX BASED ON CUURENT WEIGHT VECTOR PASSED TO FUNCTION\n",
    "    \n",
    "    y_pred = X.dot(w)\n",
    "    \n",
    "    #CALCULATING THE GRADIENT FOR LINEAR REGRESSION\n",
    "    #LOSS FUNCTION IS 1/2(Y-y_pred)^2\n",
    "    \n",
    "    grad_w = -(Y - y_pred).dot(X)\n",
    "    \n",
    "    #RETURNING THE AVERAGE GRADIENT VECTOR\n",
    "    \n",
    "    return grad_w/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENT DESCENT FUNCTION FOR LINEAR REGRESSION\n",
    "\n",
    "def gradient_descent(gradient,init_,learn_rate,n_iter=100,tol=1e-06):\n",
    "    \n",
    "    #INITIALIZING THE WEIGHT VECTOR\n",
    "    \n",
    "    w=init_\n",
    "    \n",
    "    #UNTIL THE NO OF ITERATIONS \n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        #GRADIENT FUNCTION CALCULATES GRADIENT FOR LINEAR REGRESSION\n",
    "        \n",
    "        delta= -learn_rate * gradient(w,X,Y)\n",
    "        \n",
    "        #IF DELTA IS <= TOL THEN BREAK THE LOOP(SINCE THE UPDATES ARE VERY SMALL i.e CONVERGED)\n",
    "        \n",
    "        if np.all(np.abs(delta)<=tol):\n",
    "            break\n",
    "        \n",
    "        #WEIGHT VECTOR UPDATING\n",
    "        \n",
    "        w+=delta\n",
    "    \n",
    "    #RETURNING WEIGHT VECTOR\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.c using gradient descent to find the optimal parameters relating X with Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERATING ARTIFICIAL DATA FOR REGRESSION\n",
    "\n",
    "np.random.seed(0)\n",
    "X=2.5 * np.random.randn(1000000)+1.5\n",
    "res=1.5* np.random.randn(1000000)\n",
    "Y=2+ 0.3*X + res\n",
    "\n",
    "#INSERTING CONSTANT ONES FOR BIAS WEIGHTS TO INPUT FEATURE MATRIX\n",
    "\n",
    "X= X[..., np.newaxis]\n",
    "X = np.insert(X, 0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal parameters relating X with Y are: a:0.30037444363643423  b:1.999888927889675\n",
      "we get these optimal weights at learning rate:0.2 and mse corresponding to these weights is:1.1224947962169465\n",
      "--- 12.645088195800781 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#CREATING VARIABLES TO STORE MINIMUM MSE AND WEIGHTS CORRESPONDING TO THAT MINIMUM MSE\n",
    "\n",
    "minimum_mse=100000\n",
    "opt_w=[0]\n",
    "optimal_lr=0\n",
    "\n",
    "#STROING THE TIME AT THIS POINT OF EXECTION TO CALCULATE TIME TAKEN TO RUN THE BELOW CODE\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#ITERATING THROUGH DIFFERENT LEARNING RATES\n",
    "\n",
    "for i in [0.2,0.02,0.002]:\n",
    "    \n",
    "    #CALLING GRADIENT DESCENT FUNCTION FOR LINEAR REGRESSION WITH THAT ITERATIONS LEARNING RATE\n",
    "    #AND STORING THE WEIGHT VECTOR RETURNED IN current_w\n",
    "    \n",
    "    current_w=gradient_descent(gradient, init_=[0,0], learn_rate=i,n_iter=300)\n",
    "    \n",
    "    #CALCULATING THE CURRENT PREDICTED Y VALUES BASED ON THE current_w\n",
    "    \n",
    "    current_y_pred = X.dot(current_w)\n",
    "    \n",
    "    #CALCULATING CURRENT MSE BASED ON CURRENT PREDICTED Y VALUES\n",
    "    \n",
    "    current_mse = np.mean(0.5*(Y - current_y_pred)**2 )\n",
    "    \n",
    "    #IF CURRENT MSE IS LESS THAN MINIMUM MSE AND CURRENT MSE IS NOT NA\n",
    "    #WE UPDATE THE MINIMUM MSE AS CURRENT MSE AND WEIGHTS CORRESPONDING TO MINIMUM MSE AS OPTIMAL WEIGHTS AND\n",
    "    #THE LEARNING RATE CORRESPONDING TO CURRENT MSE AS OPTIMAL LEARNING RATE\n",
    "    \n",
    "    if current_mse!=math.nan and current_mse<=minimum_mse:\n",
    "        minimum_mse=current_mse\n",
    "        opt_w[0]=current_w\n",
    "        optimal_lr=i\n",
    "        \n",
    "#PRINTING THE OPTIMAL WEIGHTS VECTOR i.e(a,b) AND OPTIMAL LEARNING RATE CORRESPONDING TO MIN MSE\n",
    "\n",
    "print('optimal parameters relating X with Y are: a:'+str(opt_w[0][1])+'  b:'+str(opt_w[0][0])) \n",
    "print('we get these optimal weights at learning rate:'+str(optimal_lr)+' and mse corresponding to these weights is:'+str(minimum_mse))\n",
    "\n",
    "#STORING THE TIME TAKEN TO RUN GRADIENT DESCENT WITH 300 EPOCHES IN gd_time\n",
    "\n",
    "gd_time=time.time() - start_time\n",
    "\n",
    "#PRINTING THE TIME TAKEN TO RUN GRADIENT DESCENT FOR LINEAR REGRESSION\n",
    "\n",
    "print(\"--- %s seconds ---\" % (gd_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.d  Implementing minibatch stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADIENT FUCTION FOR MINIBATCH SGD(SAME AS GRADIENT FUNCTION FOR GRADIENT DESCENT)\n",
    "\n",
    "def gradient(w,X,Y):\n",
    "    \n",
    "    #CALCULATING THE PREDICTED OUTPUT MATRIX BASED ON CUURENT WEIGHT VECTOR PASSED TO FUNCTION\n",
    "    \n",
    "    y_pred = X.dot(w)\n",
    "    \n",
    "    #CALCULATING THE GRADIENT FOR LINEAR REGRESSION\n",
    "    #LOSS FUNCTION IS 1/2(Y-y_pred)^2\n",
    "    \n",
    "    grad_w = -(Y - y_pred).dot(X)\n",
    "    \n",
    "    #RETURNING THE AVERAGE GRADIENT VECTOR\n",
    "    \n",
    "    return grad_w/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MINI BATCH SGD FUNCTION FOR MINI BATCH SGD\n",
    "\n",
    "def minibatch_sgd(gradient,mini_batches,init_,learn_rate,tol=1e-06,epoches=50):\n",
    "    \n",
    "    #INITIALIZING THE WEIGHT VECTOR\n",
    "    \n",
    "    wgt=init_\n",
    "    \n",
    "    #CALCULATING TOTAL NO OF MINI BATCHES\n",
    "    \n",
    "    total_batches=len(mini_batches)\n",
    "    \n",
    "    #STORING CURRENT BATCH NUMBER AND EPOCH NUMBER IN VARIABLES\n",
    "    \n",
    "    batch_no=0\n",
    "    epoch_no=0\n",
    "    \n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        #MINI BATCHES IS A LIST CONTAINING ALL THE BATCHES\n",
    "        \n",
    "        #GETS ONLY THE BATCH CORRESPONDING TO THAT batch_no FROM MINI BATCHES LIST\n",
    "        \n",
    "        batch=mini_batches[batch_no]\n",
    "        \n",
    "        #batch[0],batch[1] CONTAINS INPUT FEATURE MATRIX AND OUTPUT MATRIX CORRESPONDING TO THAT MINI BATCH\n",
    "        \n",
    "        #GRADIENT FUNCTION RETURN THE GRADIENT CORRESPONDING TO THAT MINI BATCH\n",
    "        \n",
    "        delta= -learn_rate * gradient(wgt,batch[0],batch[1])\n",
    "        \n",
    "        #IF DELTA IS <= TOL THEN BREAK THE LOOP(SINCE THE UPDATES ARE VERY SMALL I.E CONVERGED)\n",
    "        \n",
    "        if np.all(np.abs(delta)<=tol) :\n",
    "            break\n",
    "            \n",
    "        #WEIGHT VECTOR UPDATING\n",
    "        \n",
    "        wgt+=delta\n",
    "        \n",
    "        #IF ANY OF THE WEIGHTS IS NA THEN BREAK THE LOOP\n",
    "        \n",
    "        if pd.isna(float(wgt[0])) or pd.isna(float(wgt[1])):\n",
    "            break\n",
    "        \n",
    "        #INCREASING THE BATCH NO BY 1 (SO THAT IN NEXT ITERATION GRADIENT IS CALCULATED FOR NEXT BATCH)\n",
    "        \n",
    "        batch_no=(batch_no+1)%total_batches\n",
    "        \n",
    "        #IF WE REACH THE BATCH NO '0' AGAIN THEN IT MEANS 1 EPOCH IS COMPLETED\n",
    "        #SO INCREASING THE EPOCH BY 1 IF BATCH NO 'O' IS ITERATED AGAIN\n",
    "        \n",
    "        if batch_no==0:\n",
    "            epoch_no+=1\n",
    "            \n",
    "        #IF THE NO OF EPOCHES DONE ARE 10 THEN BREAK THE LOOP\n",
    "        \n",
    "        if epoch_no==epoches:\n",
    "            break\n",
    "            \n",
    "    #RETURNING WEIGHT VECTOR\n",
    "    \n",
    "    return wgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TO GENERATE BATCHES WHEN GIVEN INPUT AND OUTPUT MATRICES AND BATCH SIZE\n",
    "\n",
    "def batch_generator(X, Y, batch_size):\n",
    "\n",
    "    #FINDING THE TOTAL NO OF DATA POINTS\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    #LOOPING THROUGH TOTAL NO OF BATCHES\n",
    "    \n",
    "    for i in np.arange(0, n_samples, batch_size):\n",
    "        \n",
    "        #FINDING BEGINING AND ENDING INDICES OF THAT BATCH\n",
    "        \n",
    "        begin, end = i, min(i+batch_size, n_samples)\n",
    "        \n",
    "        #RETURNING THAT BATCH IN A GENERATOR VARIABLE\n",
    "        \n",
    "        yield X[begin:end], Y[begin:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal parameters relating X with Y using mini batch sgd with batch size1000 are: a:0.29828298958956334  b:1.9993468066618756\n",
      "we get these optimal weights at learning rate:0.002 and mse corresponding to these weights is:1.1225152617833036\n",
      "--- 47.87827157974243 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#CREATING VARIABLES TO STORE MINIMUM MSE AND WEIGHTS CORRESPONDING TO THAT MINIMUM MSE AND LEARNING RATE CORRESPONDING TO \n",
    "#THAT MINIMUM MSE\n",
    "\n",
    "minimum_mse=100000\n",
    "optimal_lr=0\n",
    "optimal_wgts=[0,0]\n",
    "\n",
    "\n",
    "#SHUFFLING THE INPUT AND OUPUT MATRICES AND STORING THEM IN NEW VARIABLES\n",
    "\n",
    "np.random.seed(0)\n",
    "idx = np.arange(X.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "X_new=X[idx]\n",
    "Y_new=Y[idx]\n",
    "\n",
    "#CREATING A LIST TO STORE MINI BATCHES\n",
    "\n",
    "mini_batches=[]\n",
    "\n",
    "#GIVING BATCH SIZE AS 1000\n",
    "\n",
    "batch_size=1000\n",
    "\n",
    "#CALLING BATCH GENERATOR FUNCTION WITH SHUFFELED MATRICES AND BATCH SIZE AS 1000\n",
    "#AND APPENDING THE MINI BATCHES RETURNED BY BATCH GENERATOR FUNCTION INTO mini batches LIST\n",
    "\n",
    "for mini_batch in batch_generator(X_new,Y_new,batch_size):\n",
    "    mini_batches.append(mini_batch)\n",
    "\n",
    "#STROING THE TIME AT THIS POINT OF EXECTION TO CALCULATE TIME TAKEN TO RUN THE MINI BATCH SGD\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#ITERATING THROUGH DIFFERENT LEARNING RATES\n",
    "\n",
    "for learning_rate in [0.2,0.02,0.002]:\n",
    "       \n",
    "        #CALLING GRADIENT DESCENT CORRESPONDING TO MINI BATCH SGD WITH EPOCHES SET TO 300\n",
    "        #STORING THE WEIGHT VECTOR RETURNED BY GRADIENT DESCENT FUNCTION IN current_w\n",
    "        \n",
    "        current_w=minibatch_sgd(gradient,mini_batches, init_=[0,0], learn_rate=learning_rate,epoches=300)\n",
    "        \n",
    "        #CALCULATING THE CURRENT PREDICTED Y VALUES BASED ON THE current_w\n",
    "        \n",
    "        current_y_pred = X.dot(current_w)\n",
    "        \n",
    "        #CALCULATING THE CURRENT MSE BASED ON CURRENT PREDICTED Y VALUES\n",
    "        \n",
    "        current_mse = np.mean(0.5 * (Y - current_y_pred)**2 )\n",
    "        \n",
    "        \n",
    "        #IF CURRENT MSE IS LESS THAN MINIMUM MSE AND CURRENT MSE IS NOT NA\n",
    "        #WE UPDATE THE MINIMUM MSE AS CURRENT MSE AND WEIGHTS CORRESPONDING TO MINIMUM MSE AS OPTIMAL WEIGHTS\n",
    "        #AND THE LEARNING RATE CORRESPONDING TO CURRENT MSE AS OPTIMAL LEARNING RATE\n",
    "        \n",
    "        if current_mse!=math.nan and current_mse<=minimum_mse:\n",
    "            minimum_mse=current_mse\n",
    "            optimal_wgts=current_w\n",
    "            optimal_lr=learning_rate\n",
    "\n",
    "#PRINTING THE OPTIMAL WEIGHTS VECTOR i.e(a,b) AND LEARNING RATE CORRESPONDING TO MIN MSE\n",
    "\n",
    "print('optimal parameters relating X with Y using mini batch sgd with batch size'+str(batch_size)+' are: a:'+str(optimal_wgts[1])+'  b:'+str(optimal_wgts[0])) \n",
    "print('we get these optimal weights at learning rate:'+str(optimal_lr)+' and mse corresponding to these weights is:'+str(minimum_mse))\n",
    "\n",
    "\n",
    "\n",
    "#STORING THE TIME TAKEN TO RUN MINIBATCH SGD WITH 300 EPOCHES IN minisgd_time\n",
    "\n",
    "minisgd_time=time.time() - start_time\n",
    "\n",
    "#PRINTING THE TIME TAKEN TO RUN MINI BATCH SGD WITH BATCH SIZE 1000 FOR LINEAR REGRESSION\n",
    "\n",
    "print(\"--- %s seconds ---\" % (minisgd_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.e implementing sgd(mini batch sgd with batch size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal parameters relating X with Y using mini batch sgd with batch size 1 are: a:0.29214498013736956  b:2.0107733074843273\n",
      "we get these optimal weights at learning rate:0.002 and mse corresponding to these weights is:1.1227074538878257\n",
      "--- 2.279576301574707 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#CREATING VARIABLES TO STORE MINIMUM MSE AND WEIGHTS CORRESPONDING TO THAT MINIMUM MSE AND LEARNING RATE CORRESPONDING TO \n",
    "#THAT MINIMUM MSE\n",
    "\n",
    "minimum_mse=100000\n",
    "optimal_lr=0\n",
    "optimal_wgts=[0,0]\n",
    "\n",
    "\n",
    "#SHUFFLING THE INPUT AND OUPUT MATRICES AND STORING THEM IN NEW VARIABLES\n",
    "\n",
    "np.random.seed(0)\n",
    "idx = np.arange(X.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "X_new=X[idx]\n",
    "Y_new=Y[idx]\n",
    "\n",
    "#CREATING A LIST TO STORE MINI BATCHES\n",
    "\n",
    "mini_batches=[]\n",
    "\n",
    "#GIVING BATCH SIZE AS 1(SGD)\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "#CALLING BATCH GENERATOR FUNCTION WITH SHUFFELED MATRICES AND BATCH SIZE AS 1000\n",
    "#AND APPENDING THE MINI BATCHES RETURNED BY BATCH GENERATOR FUNCTION INTO mini batches LIST\n",
    "\n",
    "for mini_batch in batch_generator(X_new,Y_new,batch_size):\n",
    "    mini_batches.append(mini_batch)\n",
    "\n",
    "#STROING THE TIME AT THIS POINT OF EXECTION TO CALCULATE TIME TAKEN TO RUN THE MINI BATCH SGD\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#ITERATING THROUGH DIFFERENT LEARNING RATES\n",
    "\n",
    "for learning_rate in [0.2,0.02,0.002]:\n",
    "       \n",
    "        #CALLING GRADIENT DESCENT CORRESPONDING TO MINI BATCH SGD WITH EPOCHES SET TO 300\n",
    "        #STORING THE WEIGHT VECTOR RETURNED BY GRADIENT DESCENT FUNCTION IN current_w\n",
    "        \n",
    "        current_w=minibatch_sgd(gradient,mini_batches, init_=[100,100], learn_rate=learning_rate,epoches=300)\n",
    "        \n",
    "        #CALCULATING THE CURRENT PREDICTED Y VALUES BASED ON THE current_w\n",
    "        \n",
    "        current_y_pred = X.dot(current_w)\n",
    "        \n",
    "        #CALCULATING THE CURRENT MSE BASED ON CURRENT PREDICTED Y VALUES\n",
    "        \n",
    "        current_mse = np.mean(0.5 * (Y - current_y_pred)**2 )\n",
    "        \n",
    "        \n",
    "        #IF CURRENT MSE IS LESS THAN MINIMUM MSE AND CURRENT MSE IS NOT NA\n",
    "        #WE UPDATE THE MINIMUM MSE AS CURRENT MSE AND WEIGHTS CORRESPONDING TO MINIMUM MSE AS OPTIMAL WEIGHTS\n",
    "        #AND THE LEARNING RATE CORRESPONDING TO CURRENT MSE AS OPTIMAL LEARNING RATE\n",
    "        \n",
    "        if current_mse!=math.nan and current_mse<=minimum_mse:\n",
    "            minimum_mse=current_mse\n",
    "            optimal_wgts=current_w\n",
    "            optimal_lr=learning_rate\n",
    "\n",
    "#PRINTING THE OPTIMAL WEIGHTS VECTOR i.e(a,b) AND LEARNING RATE CORRESPONDING TO MIN MSE\n",
    "\n",
    "print('optimal parameters relating X with Y using mini batch sgd with batch size '+str(batch_size)+' are: a:'+str(optimal_wgts[1])+'  b:'+str(optimal_wgts[0])) \n",
    "print('we get these optimal weights at learning rate:'+str(optimal_lr)+' and mse corresponding to these weights is:'+str(minimum_mse))\n",
    "\n",
    "#STORING TIME TAKEN TO RUN SGD IN sgd_time\n",
    "\n",
    "sgd_time=time.time() - start_time\n",
    "\n",
    "#PRINTING THE TIME TAKEN TO RUN MINI BATCH SGD WITH BATCH SIZE 1 FOR LINEAR REGRESSION\n",
    "\n",
    "print(\"--- %s seconds ---\" % (sgd_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.365511894226074 45.598695278167725\n"
     ]
    }
   ],
   "source": [
    "#PRINTING THE TIME DIFFERENCE BETWEEN RUNNING GRADIENT DESCENT AND SGD AND MINI BATCH SGD WITH BATCH SIZE 1000 AND SGD\n",
    "\n",
    "print(gd_time-sgd_time , minisgd_time-sgd_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see that time taken by sgd with 300 epoches is less than gd with 300 epoches.\n",
    "and also time taken by sgd with 300 epoches is less than mini batch sgd with batch size 1000 and 300 epoches\n",
    "\n",
    "SO SGD DOES BETTER IN TERMS OF TIME ON OUR DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.e finding optimal batch size that works best(HYPERPARAMETER TUNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TO SHUFFLE DATA\n",
    "\n",
    "def shuffle_data(X, Y):\n",
    "    np.random.seed(0)\n",
    "    idx = np.arange(X.shape[0])\n",
    "    \n",
    "    #SHUFFLING THE DATA\n",
    "    \n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    #RETURNING SHUFFLED DATA\n",
    "    \n",
    "    return X[idx], Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TO SPLIT OUR DATA INTO TRAIN AND TEST SETS\n",
    "\n",
    "def train_test_split(X, Y, test_size=0.3):\n",
    "    \n",
    "    #CALLING SHUFFLE FUNCTION TO SHUFFLE THE DATA BEFORE SPLITTING\n",
    "    \n",
    "    X, Y = shuffle_data(X, Y)\n",
    "    \n",
    "    #SPLITTING THE DATA INTO TRAIN AND TEST IN THE RATIO SPECIFIED\n",
    "    \n",
    "    split_i = len(Y) - int(len(Y) // (1 / test_size))\n",
    "    X_train, X_test = X[:split_i], X[split_i:]\n",
    "    Y_train, Y_test = Y[:split_i], Y[split_i:]\n",
    "\n",
    "    #RETURNING THE SPLIT DATA\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE DATA INTO TEST AND TRAIN BY CALLING train_test_split FUNCTION WITH TEST SIZE RATIO AS 0.3 \n",
    "    \n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal batch size is:401 learning rate:0.002 weight vector:[1.99911664 0.29972309] mse on test data: 1.1260119886838889\n"
     ]
    }
   ],
   "source": [
    "#CREATING VARIABLES TO STORE MINIMUM MSE ON TEST SET AND OPTIMAL BATCH SIZE,WEIGHTS ,LEARNING RATE \n",
    "#CORRESPONDING TO THAT MINIMUM MSE \n",
    "\n",
    "optimal_lr=0\n",
    "optimal_wgts=[0,0]\n",
    "minimum_mse=100000000\n",
    "optimal_batchsize=0\n",
    "\n",
    "#LOOPING THROUGH DIFFERENT BATCH SIZES\n",
    "\n",
    "for batch_size in range(1,1000,50):\n",
    "    \n",
    "    #SHUFFLING THE TRAIN DATA\n",
    "    \n",
    "    X_new, Y_new = shuffle_data(X_train, Y_train)\n",
    "    \n",
    "    #CREATING A LIST TO STORE MINI BATCHES\n",
    "    \n",
    "    mini_batches=[]\n",
    "    \n",
    "    #CALLING BATCH GENERATOR FUNCTION WITH SHUFFELED MATRICES AND BATCH SIZE AS 1000\n",
    "    #AND APPENDING THE MINI BATCHES RETURNED BY BATCH GENERATOR FUNCTION INTO mini batches LIST\n",
    "    \n",
    "    for mini_batch in batch_generator(X_new,Y_new,batch_size):\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    #LOOPING THROUGH DIFFERENT LEARNING RATES\n",
    "    \n",
    "    for learning_rate in [0.2,0.02,0.002,0.0002,0.00002]:\n",
    "        \n",
    "        #CALLING GRADIENT DESCENT CORRESPONDING TO MINI BATCH SGD WITH EPOCHES SET TO 10\n",
    "        #STORING THE WEIGHT VECTOR RETURNED BY GRADIENT DESCENT FUNCTION IN current_w\n",
    "        \n",
    "        current_w=minibatch_sgd(gradient,mini_batches, init_=[100,100], learn_rate=learning_rate,epoches=10)\n",
    "        \n",
    "        #CALCULATING THE CURRENT PREDICTED Y VALUES OF TEST DATA BASED ON THE current_w\n",
    "        \n",
    "        current_y_pred = X_test.dot(current_w)\n",
    "        \n",
    "        #CALCULATING THE CURRENT MSE BASED ON CURRENT PREDICTED Y VALUES OF TEST DATA\n",
    "        \n",
    "        current_mse = np.mean(0.5 * (Y_test - current_y_pred)**2 )\n",
    "        \n",
    "        #IF CURRENT MSE IS LESS THAN MINIMUM MSE AND CURRENT MSE IS NOT NA\n",
    "        #WE UPDATE THE MINIMUM MSE AS CURRENT MSE AND WEIGHTS CORRESPONDING TO MINIMUM MSE AS OPTIMAL WEIGHTS\n",
    "        #AND THE LEARNING RATE CORRESPONDING TO CURRENT MSE AS OPTIMAL LEARNING RATE\n",
    "        #AND THE BATCH SIZE CORRESPONDING TO CUURENT MSE AS OPTIMAL BATCH SIZE\n",
    "        \n",
    "        if current_mse!=math.nan and current_mse<=minimum_mse:\n",
    "            minimum_mse=current_mse\n",
    "            optimal_wgts=current_w\n",
    "            optimal_lr=learning_rate\n",
    "            optimal_batchsize=batch_size\n",
    "\n",
    "#PRINTING OPTIMAL BATCH SIZE AND LEARNING RATE TUNED TO THAT OPTIMAL BATCH SIZE AND WEIGHT VECTOR CORRESPONDING TO IT\n",
    "#AND ITS MSE ON TEST DATA\n",
    "\n",
    "print('optimal batch size is:'+str(optimal_batchsize)+\n",
    "       ' learning rate:'+str(optimal_lr)+\n",
    "      ' weight vector:'+str(optimal_wgts)+\n",
    "      ' mse on test data: '+str(minimum_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.as you can see the optimal batch size is :401\n",
    "\n",
    "2.but this optimal batch size is not a constant ,if our data is shuffled differently before making mini batches then you may\n",
    "get different optimal batch size\n",
    "\n",
    "3.as the mini batch size gets higher the generalization error also gets higher as it over fits the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2(i)probability that someone has both cold and a fever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ P(cold \\wedge fever) = P( fever | cold ) P(cold)  $\n",
    "## $P( fever | cold ) = 0.307$  -   from the table\n",
    "## $P(cold) = 0.02$            - from the table\n",
    "## $ P(cold \\wedge fever) = (0.307)*(0.02)  $\n",
    "## $ P(cold \\wedge fever) =  0.00614 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2(ii) probability that someone who has a cough has a cold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $P(cold|cough) = \\frac{P(cough \\wedge cold)}{P(cough)}$\n",
    "\n",
    "## To calculate $P(cough \\wedge cold)$ we require $P(lung disease)$ first ,so let us calculate it\n",
    "### $P(lung disease) = P(lung disease|smokes)P(smokes) + P(lung disease|\\overline{smokes})P(\\overline{smokes})$\n",
    "#### all the above values can be directly found in the Bayesian network ,so taking those values and substituting\n",
    "### $P(lung disease) = 0.1009*0.2 + 0.001*0.8$\n",
    "### $P(lung disease) = 0.02018 + 0.0008$\n",
    "### $P(lung disease) = 0.02098$\n",
    "### we get $P(lung disease) = 0.02098$\n",
    "## now let us calculate  $P(cough \\wedge cold)$\n",
    "### $P(cough \\wedge cold) = P(cough \\wedge cold \\wedge lungdisease) + P(cough \\wedge cold \\wedge \\overline{lungdisease})$\n",
    "### $=P(cough|cold,lungdisease)P(cold)P(lungdisease)+P(cough|cold,\\overline{lungdisease})P(cold)P(\\overline{lu  ngdisease})$\n",
    "#### except $P(lung disease)$ all other values can be directly found in Bayesian network and we have already calculated $P(lung disease)$\n",
    "\n",
    "### $=0.7525*0.02*0.02098+0.505*0.02*(1-0.02098)$\n",
    "### $=0.010203851$\n",
    "### we get $P(cough \\wedge cold) =0.010203851$\n",
    "## now let us calculate  $P(cough)$\n",
    "### $P(cough) = P(cough|cold,lungdisease)P(cold)P(lungdisease)+P(cough|\\overline{cold},lungdisease)P(\\overline{cold})P(lungdisease)+P(cough|cold,\\overline{lungdisease})P(cold)P(\\overline{lungdisease})+P(cough|\\overline{cold},\\overline{lungdisease})P(\\overline{cold})P(\\overline{lungdisease})$\n",
    "#### all the above values can be directly found in Bayesian network except $P(lung disease)$ ,by substituting them we get\n",
    "### $P(cough) = 0.7525*0.02*0.02098 + 0.505*0.98*0.02098 + 0.505*0.02*0.97902 + 0.01*0.98*0.97902$\n",
    "### $P(cough) = 0.000315749 + 0.010383002 + 0.009888102 + 0.009594396$\n",
    "### $P(cough) = 0.030181249$\n",
    "### we get  $P(cough) = 0.030181249$\n",
    "## now we have both $P(cough \\wedge cold) =0.010203851$ and $P(cough) = 0.030181249$ \n",
    "### $P(cold|cough) = \\frac{P(cough \\wedge cold)}{P(cough)}$\n",
    "### $P(cold|cough) = \\frac{0.010203851}{0.030181249}$\n",
    "### $P(cold|cough) = 0.338085776$\n",
    "## finally we get $P(cold|cough) = 0.338085776$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.MLE for the parameters of a k-sided multinomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let The probabilities of the observations are as follows:\n",
    "### $p_{1}= P(X_{1}) = \\frac{x_{1}}{n}$\n",
    "### $p_{2}= P(X_{2}) = \\frac{x_{2}}{n}$\n",
    "### ....\n",
    "### $p_{k}= P(X_{k}) = \\frac{x_{k}}{n}$\n",
    "### The likelihood of k sided multinomial distribution as a joint probability $p$ is\n",
    "### $L(p) = \\frac{n!}{x_{1}!x_{2}!..x_{k}!}\\prod_{i=1}^{k} p_{i}^{x_{i}}$\n",
    "### $L(p) = n!\\prod_{i=1}^{k} \\frac{p_{i}^{x_{i}}}{x_{i}!}$\n",
    "### Applying log likelihood for the above\n",
    "### $\\log L(p) = \\log \\left(n!\\prod_{i=1}^{k} \\frac{p_{i}^{x_{i}}}{x_{i}!}\\right)$\n",
    "###                $\\hspace{2.15cm}= \\log n! + \\log \\prod_{i=1}^{k}\\frac{p_{i}^{x_{i}}}{x_{i}!}$\n",
    "###                $\\hspace{2.15cm}= \\log n! + \\sum_{i=1}^k \\log\\frac{p_{i}^{x_{i}}}{x_{i}!}$\n",
    "### $\\hspace{2.15cm}=\\log n! + \\sum_{i=1}^k x_i \\log p_i - \\sum_{i=1}^k \\log x_{i}!$\n",
    "### Let  $\\hspace{0.25cm}l(p) = \\log L(p)$\n",
    "### posing a constraint ( $\\sum_{i=1}^k {p_{i}}=1$) with lagrange multiplier\n",
    "### $l^\\prime(p,\\lambda) = l(p) + \\lambda \\left(1-\\sum_{i=1}^{k} p_{i}\\right)$\n",
    "### $l^\\prime(p,\\lambda) =\\log n! + \\sum_{i=1}^k x_i \\log p_i - \\sum_{i=1}^k \\log x_{i}!+\\lambda \\left(1-\\sum_{i=1}^{k} p_{i}\\right)$\n",
    "### Now to find argmax for $l^\\prime(p,\\lambda)$, lets differentitate w.r.t to $p$\n",
    "### $\\frac{\\partial {l^\\prime(p,\\lambda)}}{\\partial p_{i}} = \\frac{\\partial {}}{\\partial p_{i}}\\log n!+\\frac{\\partial {}}{\\partial p_{i}}\\sum_{i=1}^k x_i \\log p_i-\\frac{\\partial {}}{\\partial p_{i}}\\sum_{i=1}^k \\log x_{i}!+\\frac{\\partial {}}{\\partial p_{i}}\\lambda \\left(1-\\sum_{i=1}^{k} p_{i}\\right)=0$\n",
    "### $\\hspace{1.73cm}= 0+\\frac{\\partial {}}{\\partial p_{i}}\\sum_{i=1}^k x_i \\log p_i-0+\\frac{\\partial {}}{\\partial p_{i}}\\lambda \\left(1-\\sum_{i=1}^{k} p_{i}\\right)$\n",
    "### $\\frac{\\partial {}}{\\partial p_{i}}\\sum_{i=1}^k x_i \\log p_i-\\lambda\\frac{\\partial{}}{\\partial{p_{i}}}\\sum_{i=1}^{k}p_{i}=0$\n",
    "### $\\frac{x_{i}}{p_{i}}-\\lambda=0$\n",
    "## $p_{i} = \\frac{x_{i}}{\\lambda}$\n",
    "### Since, the summation over all probabilities should sum to 1\n",
    "### $\\sum_{i=1}^{k}p_i = \\sum_{i=1}^{k}\\frac{x_{i}}{\\lambda}=1$\n",
    "### $\\sum_{i=1}^{k}\\frac{x_{i}}{\\lambda}=1$\n",
    "### $\\frac{1}{\\lambda} \\sum_{i=1}^{k} x_{i} = 1$\n",
    "### we know that $\\hspace{0.25cm}\\sum_{i=1}^{k} x_{i} = n$\n",
    "### $\\lambda = n$\n",
    "### so $\\hspace{0.5cm}p_i = \\frac{x_{i}}{n}$\n",
    "### The probability distribution parameters for MLE of k-sided multinomial distribution is\n",
    "## $p = \\left(\\frac{x_1}{n},\\frac{x_2}{n},\\frac{x_3}{n},...,\\frac{x_k}{n}\\right)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
